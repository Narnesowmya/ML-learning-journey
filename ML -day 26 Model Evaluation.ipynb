{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e969cf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Model Evaluation: This module provides functions to evaluate machine learning models using various metrics.\n",
    "It includes functions to calculate accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843cc40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"1.Train-Test Split\n",
    "2.Cross-Validation\n",
    "3confusion Matrix\n",
    "4.Roc-Auc Curve\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2d5515",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"1.train-test split\n",
    "why we need train-test split?\n",
    "to evaluate the performance of our machine learning model on unseen data and to prevent overfitting.\n",
    "we split the dataset into two parts: a training set and a testing set.\n",
    "the training set is used to train the model, while the testing set is used to evaluate its performance.\n",
    "this helps us to understand how well the model generalizes to new, unseen data.\n",
    "the typical ratio for train-test split is 80:20 or 70:30, where 80% or 70% of the data is used for training and the remaining 20% or 30% is used for testing.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8986efe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"What is single split and why it is not reliable?\n",
    "Single split refers to dividing the dataset into a single training set and a single testing set. While this method is straightforward, it has some limitations that make it less reliable for evaluating model performance:\n",
    "1.Variability: A single split may not represent the entire dataset well, leading to high variability in performance metrics. The model's performance can vary significantly depending on which data points end up in the training and testing sets.\n",
    "2.Overfitting Risk: If the model is tuned based on the performance on a single test set, it may lead to overfitting to that specific test set, resulting in poor generalization to new data.\n",
    "3.Lack of Robustness: A single split does not provide insights into how the model performs\n",
    "across different subsets of the data, making it difficult to assess its robustness.\n",
    "To address these issues, techniques like cross-validation are often used, where the dataset is divided into multiple subsets, and the model is trained and evaluated multiple times to obtain a more reliable estimate of its performance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dc9a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Proper train–test split for classification\n",
    "Key rules:\n",
    "\n",
    "Keep test data unseen\n",
    "\n",
    "Maintain class balance\n",
    "\n",
    "Fix randomness\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313f512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4877722",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"2.CROSS-VALIDATION\n",
    "Cross-validation is a technique used to assess the performance and generalization ability of a machine learning model. It involves dividing the dataset into multiple subsets or \"folds\" and training and evaluating the model on different combinations of these folds. The most common form of cross-validation is k-fold cross-validation, where the dataset is divided into k equal-sized folds. The model is trained on k-1 folds and tested on the remaining fold. This process is repeated k times, with each fold serving as the test set once. The final performance metric is typically averaged over all k iterations to provide a more robust estimate of the model's performance. Cross-validation helps to mitigate issues such as overfitting and provides insights into how well the model generalizes to unseen data.\n",
    "rain–test split still depends on:\n",
    "\n",
    "Randomness\n",
    "\n",
    "One snapshot\n",
    "“How stable is my model across different samples?”\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8289d491",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"What is K-Fold Cross-Validation?\n",
    "K-Fold Cross-Validation is a resampling technique used to evaluate the performance of a machine learning model. In this method, the dataset is divided into 'k' equal-sized subsets or \"folds\". The model is trained and validated 'k' times, each time using a different fold as the validation set and the remaining 'k-1' folds as the training set. This process ensures that every data point is used for both training and validation exactly once. After all iterations, the performance metrics (such as accuracy, precision, recall, etc.) from each fold are averaged to provide a more reliable estimate of the model's overall performance. K-Fold Cross-Validation helps to reduce variability in model evaluation and provides insights into how well the model generalizes to unseen data.\n",
    "Example: 5-Fold CV\n",
    "\n",
    "Split data into 5 equal parts\n",
    "\n",
    "Train on 4 parts\n",
    "\n",
    "Test on 1 part\n",
    "\n",
    "Repeat 5 times\n",
    "\n",
    "Average performance\n",
    "\n",
    "Each data point:\n",
    "\n",
    "Used for training\n",
    "\n",
    "Used for testing\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c155377",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" cross validation is used in logistic regression, decision tree, random forest, support vector machine, etc. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ed671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion - matrix\n",
    "\"\"\" why confusion matrix?\n",
    "A confusion matrix is a useful tool for evaluating the performance of a classification model. It provides a\n",
    "detailed breakdown of the model's predictions compared to the actual outcomes, allowing us to see not only the overall accuracy but also the types of errors the model is making. By analyzing the confusion matrix, we can identify patterns in misclassifications, assess the model's ability to distinguish between different classes, and make informed decisions about model improvements or adjustments. It is particularly valuable in scenarios where class imbalance exists, as it helps to understand how well the model performs on each class individually.\n",
    "Confusion Matrix Structure:\n",
    "A confusion matrix is a table that summarizes the performance of a classification model by comparing the predicted labels\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b22fd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Why confusion matrix is powerful\n",
    "\n",
    "It tells:\n",
    "\n",
    "What type of mistakes model makes\n",
    "\n",
    "Whether mistakes are acceptable\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d1f11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Metric\tMeaning\n",
    "Precision\tHow reliable positive predictions are\n",
    "Recall\tHow many positives we captured\n",
    "F1-score\tBalance between precision & recall\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919782aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.Roc-Auc Curve\n",
    "\"\"\"\n",
    " Classification models:\n",
    "\n",
    "Output probabilities\n",
    "\n",
    "Need a threshold (default = 0.5)\n",
    "\n",
    "ROC evaluates:\n",
    "\n",
    "“How well does the model separate classes across ALL thresholds?”\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"what is roc - auc curve and why it is used?\n",
    "ROC (Receiver Operating Characteristic) curve is a graphical representation used to evaluate the performance of a\n",
    "classification model at various threshold settings. It plots the True Positive Rate (sensitivity) against the False Positive Rate (1-specificity) at different threshold levels. The ROC curve helps to visualize the trade-off between sensitivity and specificity, allowing us to assess how well the model can distinguish between positive and negative classes.\n",
    "The Area Under the Curve (AUC) is a single scalar value that summarizes the overall performance\n",
    "of the ROC curve. It represents the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance by the model. An AUC value of 0.5 indicates no discrimination (random guessing), while a value of 1.0 indicates perfect discrimination.\n",
    "ROC-AUC is used because it provides a comprehensive evaluation of a model's performance across all possible classification thresholds, making it particularly useful for imbalanced datasets and scenarios where the costs of false positives and false negatives differ.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944679cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"AUC summarizes ROC curve into one number.\n",
    "\n",
    "AUC\tMeaning\n",
    "0.5\tRandom\n",
    "0.6–0.7\tWeak\n",
    "0.7–0.8\tGood\n",
    "0.8–0.9\tVery good\n",
    "> 0.9\tExcellent\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a865b56f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
